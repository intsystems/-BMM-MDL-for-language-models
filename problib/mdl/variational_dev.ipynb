{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Trainer import Trainer\n",
    "\n",
    "from variational_probing import VariationalProbingModel\n",
    "from utils import *\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at D:/models/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = VariationalProbingModel(pretrained_path=\"D:/models/roberta-base\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/models/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = tokenizer([\"hello world!\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(inps[\"input_ids\"].to(\"cuda\"), inps[\"attention_mask\"].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0710,  0.0783,  0.0031,  ..., -0.0746, -0.0541,  0.0182],\n",
       "         [-0.1743, -0.2809, -0.0731,  ..., -0.3774,  0.0176,  0.3123],\n",
       "         [-0.1348,  0.0859,  0.1506,  ..., -0.1907, -0.1079,  0.3542],\n",
       "         [-0.2056, -0.2489,  0.2531,  ...,  0.0606, -0.2765,  0.7178],\n",
       "         [-0.0636,  0.0742, -0.0212,  ..., -0.1239, -0.0633,  0.0021]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.5100e-01, -4.8025e-01, -3.6435e-01, -2.2246e-01,  1.5880e-01,\n",
       "          1.6888e-01, -1.4584e-01,  3.8146e-01, -1.6976e-01, -7.0282e-01,\n",
       "          2.0111e-01,  3.9626e-02, -1.0630e-01, -5.3962e-01,  1.6174e-01,\n",
       "         -2.6259e-01,  2.3962e-01, -1.0403e-01, -3.8447e-03,  3.0312e-02,\n",
       "          1.1269e-01, -3.9087e-01, -2.8602e-01,  4.8452e-01, -4.2954e-01,\n",
       "         -1.8140e-01,  5.0511e-01,  4.2459e-01,  5.0607e-01,  2.0992e-01,\n",
       "         -3.6054e-01, -5.3043e-01,  5.8689e-01, -6.3716e-01, -2.5832e-01,\n",
       "         -1.9446e-01, -1.8976e-01,  4.5844e-01, -3.5506e-01,  6.3794e-01,\n",
       "         -3.8237e-01,  2.2757e-01, -1.0349e-01,  3.5196e-02,  7.2483e-03,\n",
       "          4.0001e-01, -3.4871e-01, -3.2596e-02,  5.0643e-01, -1.6392e-01,\n",
       "          6.5668e-01, -1.6136e-01,  7.9871e-02,  1.8791e-01,  4.7521e-01,\n",
       "         -2.0537e-01,  2.9085e-02, -3.3136e-01,  6.0828e-01, -2.3280e-01,\n",
       "          2.0384e-01, -1.4726e-01, -7.4879e-02, -7.4129e-01,  4.3283e-01,\n",
       "         -3.7077e-01, -6.2959e-01, -2.0811e-01, -6.1524e-01, -6.7024e-01,\n",
       "          7.5973e-01,  3.6352e-01,  1.0225e-01,  8.1604e-02, -3.4967e-03,\n",
       "         -1.1113e-01,  2.0911e-01,  3.3735e-01, -2.1966e-01, -6.3950e-01,\n",
       "         -2.0272e-01,  6.8035e-02, -5.6254e-01, -2.9365e-01,  3.4489e-01,\n",
       "         -5.9279e-01, -2.6912e-01, -3.9438e-02,  6.7858e-02,  4.6385e-01,\n",
       "         -3.3386e-01,  4.0147e-01, -4.8299e-01,  3.4585e-01, -3.8542e-01,\n",
       "          7.4930e-01, -9.2449e-02, -4.6616e-03, -5.3186e-01, -1.0414e-01,\n",
       "          5.7903e-02, -1.7901e-01,  4.3828e-01, -4.9661e-01, -4.1428e-03,\n",
       "         -7.3970e-01, -6.3766e-01,  4.0111e-01,  5.5584e-01,  2.6490e-01,\n",
       "         -3.3322e-01, -2.1975e-01, -4.2731e-01,  4.8633e-02, -7.4787e-03,\n",
       "          3.8978e-01,  4.0429e-02,  3.8834e-01,  4.5657e-01, -3.5467e-01,\n",
       "          3.9508e-01,  4.9771e-02,  4.8602e-01,  1.3221e-01,  3.6315e-01,\n",
       "         -2.3128e-01,  6.0462e-01, -2.2725e-01, -4.2393e-01, -1.0376e-02,\n",
       "         -1.6517e-01,  7.3579e-02,  1.7256e-02, -4.5435e-01, -1.3411e-01,\n",
       "          1.1583e-03,  2.2550e-01,  6.2309e-02,  5.6202e-01, -7.6938e-01,\n",
       "          3.0887e-01, -6.8015e-02,  4.9131e-01, -4.1743e-01, -1.0165e-01,\n",
       "          1.9239e-03,  4.9804e-01, -3.3298e-01,  4.8991e-01, -5.6877e-01,\n",
       "          2.7457e-01,  1.5617e-01,  3.5671e-01, -4.3904e-01, -7.9769e-01,\n",
       "         -3.4001e-01, -4.0611e-01,  2.5906e-02, -5.7549e-01, -4.9816e-01,\n",
       "          7.2059e-02,  2.9685e-01,  2.1921e-01,  3.1726e-02,  9.2235e-03,\n",
       "         -9.1987e-02, -4.4581e-01, -3.1862e-01, -6.1042e-01, -5.1259e-02,\n",
       "         -6.3246e-02,  2.9502e-01, -3.5917e-02,  1.1630e-01,  1.5883e-01,\n",
       "          3.0404e-01, -2.3326e-01,  5.4561e-01, -1.9596e-01,  9.2116e-03,\n",
       "          4.8351e-01, -4.0003e-01,  6.5768e-01,  5.1272e-01,  4.0878e-01,\n",
       "         -5.2012e-01,  7.5506e-02, -3.2836e-02,  3.4422e-01,  4.7236e-01,\n",
       "         -1.8689e-01,  1.7607e-01, -3.8232e-01,  5.7740e-01,  6.2848e-02,\n",
       "          3.6641e-01,  1.1986e-02,  5.2481e-01,  2.3460e-01,  7.4209e-02,\n",
       "          9.0013e-02,  2.7416e-01,  8.9953e-02,  6.1227e-02,  1.9586e-02,\n",
       "          5.3117e-03, -4.8039e-01, -6.2275e-02, -8.2010e-01, -4.9198e-01,\n",
       "         -4.3850e-01,  3.6863e-01, -1.8987e-01,  3.0682e-01, -3.6966e-01,\n",
       "         -2.2443e-01,  6.3448e-01,  4.4341e-01,  6.8365e-02,  1.1473e-01,\n",
       "          4.3747e-03, -4.0437e-01, -8.5660e-02,  2.8568e-01, -2.4111e-01,\n",
       "          7.0211e-01,  2.5347e-01,  3.5155e-01,  1.8121e-01, -1.1014e-01,\n",
       "         -5.4214e-01,  2.9368e-01,  3.2355e-01, -3.7324e-01,  7.1088e-01,\n",
       "          8.4392e-01,  1.9599e-01,  2.9348e-01,  5.5682e-02,  4.5141e-01,\n",
       "          5.0706e-01,  2.2351e-01, -4.2579e-02, -5.1719e-01,  4.6804e-01,\n",
       "         -2.8490e-01,  8.6145e-02, -3.9251e-01,  2.6732e-01,  4.2824e-01,\n",
       "         -2.4575e-01,  5.6446e-01, -7.0318e-02, -6.7547e-01,  1.2075e-01,\n",
       "         -1.1508e-02, -3.0671e-01,  3.9415e-01, -1.6920e-01,  1.4162e-01,\n",
       "         -6.3938e-01, -2.6928e-01,  2.3868e-01, -5.3029e-01,  1.8159e-02,\n",
       "         -7.2458e-01,  6.1145e-01,  1.1686e-01, -1.2953e-01,  1.6042e-01,\n",
       "          5.5314e-01,  3.9805e-01, -3.4598e-02, -4.5508e-01, -3.8566e-01,\n",
       "         -1.3246e-01, -5.7226e-02, -3.8183e-01,  7.0077e-01, -8.2592e-03,\n",
       "         -1.5436e-01,  1.7034e-02,  9.9996e-02,  4.3256e-01, -2.4099e-01,\n",
       "          3.5642e-01, -1.0750e-01,  2.6409e-01,  6.1016e-01,  8.0179e-02,\n",
       "          3.9305e-01,  3.4626e-01,  6.7106e-01, -4.0485e-01, -5.6222e-01,\n",
       "          1.3696e-01,  5.6473e-01, -2.5282e-01,  3.8354e-01, -7.8929e-02,\n",
       "         -3.1462e-02, -6.2407e-01,  1.7408e-01, -4.3375e-01,  3.1418e-01,\n",
       "          3.8296e-01, -6.4205e-02, -4.9147e-01, -1.2770e-02, -3.8402e-01,\n",
       "          2.8319e-01,  6.2653e-01, -2.8374e-01,  5.3506e-01, -2.0456e-01,\n",
       "          1.9127e-01, -2.2206e-01, -9.3216e-03,  1.2815e-01, -4.7920e-01,\n",
       "          4.3620e-01, -5.8220e-01,  3.0396e-01,  4.7694e-01, -5.6592e-01,\n",
       "          1.1802e-01, -2.7660e-01, -3.0736e-01,  1.8402e-01,  5.3146e-01,\n",
       "         -2.7151e-01,  1.0651e-01, -5.7499e-01,  2.8704e-01,  1.8200e-01,\n",
       "          2.9412e-01, -2.5458e-01, -1.4321e-01,  3.8456e-01,  1.0482e-01,\n",
       "          6.2958e-01, -4.3511e-01, -2.1895e-01,  6.8654e-01, -3.1928e-01,\n",
       "         -7.6466e-02, -4.5498e-01,  4.8242e-01, -7.3410e-01, -1.7131e-01,\n",
       "          9.5315e-02, -1.9861e-01, -3.0969e-02, -2.9071e-01,  2.8755e-01,\n",
       "         -2.8009e-01,  6.2050e-01, -1.3429e-01, -2.2402e-02,  4.1583e-01,\n",
       "          1.9389e-01, -2.1198e-01,  4.0513e-01,  1.8185e-01, -5.8838e-01,\n",
       "         -4.2716e-01, -2.8070e-01,  3.5242e-01, -1.3288e-01,  4.8294e-01,\n",
       "         -1.4881e-01,  1.5938e-01, -3.0613e-01,  4.3482e-02, -5.9204e-02,\n",
       "          3.1121e-01,  6.2220e-02, -4.9426e-01,  4.8912e-01,  5.0997e-01,\n",
       "         -1.2380e-01,  2.1023e-01, -1.6303e-01, -1.7983e-01, -3.4160e-01,\n",
       "          4.5331e-01,  1.8126e-01,  6.9362e-02,  2.4442e-02,  4.7215e-01,\n",
       "         -5.7350e-02,  3.0272e-01,  5.7638e-02,  3.7256e-01,  3.5574e-01,\n",
       "          3.2350e-01,  2.9557e-01, -1.2784e-01,  5.6908e-01,  4.2587e-01,\n",
       "         -3.8055e-01, -3.9654e-01,  2.1379e-01,  5.5744e-01, -5.5652e-01,\n",
       "          7.1802e-01, -9.8298e-02, -6.7657e-01,  1.6646e-01,  5.0491e-01,\n",
       "          2.8449e-01,  4.7971e-01,  6.4974e-01, -7.2167e-01,  1.5133e-01,\n",
       "         -5.2175e-02,  1.6487e-01, -8.5258e-01, -5.1977e-03,  3.2422e-01,\n",
       "          2.6681e-01, -3.7512e-02, -7.2115e-01,  4.2100e-01,  1.7611e-01,\n",
       "         -3.5777e-01,  3.6257e-01,  2.5577e-01, -9.6419e-02,  1.5811e-02,\n",
       "          6.5797e-02,  6.4585e-01,  3.1378e-01,  1.3935e-01, -3.6235e-01,\n",
       "          6.2383e-02,  4.3088e-01,  1.0491e-01,  8.3673e-01, -2.8843e-01,\n",
       "          5.6914e-01, -4.0555e-01,  1.2433e-01, -3.2584e-01,  1.5286e-01,\n",
       "          3.4462e-01, -4.5724e-01,  6.3130e-01, -1.3726e-01,  5.5422e-01,\n",
       "         -7.9788e-02, -2.5670e-02, -1.0389e-01,  1.8140e-01,  3.0250e-01,\n",
       "          4.6601e-01, -3.1710e-01,  1.4408e-01, -3.6377e-01,  6.1467e-01,\n",
       "         -1.3804e-01,  1.1801e-01, -7.4687e-01, -6.9093e-01,  6.4922e-01,\n",
       "          3.6024e-01, -5.0949e-01,  1.2649e-01, -3.5310e-01,  1.6894e-02,\n",
       "          1.1032e-01, -6.1925e-01, -2.3214e-01,  1.6886e-01,  1.1580e-01,\n",
       "         -1.0442e-01,  3.7162e-02,  3.9589e-01,  1.9546e-01, -2.4305e-01,\n",
       "         -2.3163e-01, -1.1863e-01, -3.4646e-01,  4.1553e-01,  1.0737e-01,\n",
       "          4.1788e-01, -5.7811e-01,  3.9399e-01, -1.8788e-01, -3.4548e-01,\n",
       "          4.5637e-01,  4.1817e-01, -3.3737e-01, -2.3499e-01, -4.2240e-01,\n",
       "          4.6634e-01,  1.3628e-01, -1.5495e-01, -5.4834e-02, -1.1290e-01,\n",
       "          2.5486e-01,  4.3640e-01, -1.7228e-01, -2.3720e-01,  8.3483e-01,\n",
       "          7.2096e-02,  7.2516e-01,  1.9718e-03,  7.5348e-01, -1.8002e-01,\n",
       "          1.0139e-01, -7.2863e-01, -1.7277e-01,  4.1738e-01,  3.2712e-01,\n",
       "         -2.9232e-01, -3.1358e-01, -3.5280e-01,  3.1440e-01,  2.6907e-02,\n",
       "         -2.5732e-01,  2.1298e-01,  5.4277e-01, -1.1087e-01, -3.7419e-01,\n",
       "          1.5590e-02, -1.7708e-01, -2.3603e-02,  1.8199e-01,  3.7463e-01,\n",
       "         -2.3743e-01, -1.3275e-02, -1.6429e-01,  1.1095e-01,  7.9735e-01,\n",
       "         -6.3319e-01,  7.2615e-01, -1.5156e-01, -3.9517e-01,  3.1549e-02,\n",
       "         -3.7962e-01,  2.4373e-01, -4.5910e-01, -1.6148e-01, -4.2138e-01,\n",
       "         -7.3555e-01, -2.0192e-01, -1.7947e-01, -3.6184e-01,  2.9112e-02,\n",
       "         -2.2644e-01,  6.9971e-01,  6.5821e-01, -3.4290e-01, -4.9822e-01,\n",
       "          3.6352e-01,  1.7084e-01,  1.7164e-01, -2.2883e-01,  2.4546e-01,\n",
       "         -3.5638e-01,  2.6771e-01, -1.7396e-01, -4.8443e-01, -2.0359e-01,\n",
       "          3.5882e-01,  4.7437e-01,  6.2457e-02, -5.2309e-01, -1.7625e-01,\n",
       "         -4.3582e-01,  2.3052e-01,  3.0989e-01, -1.1067e-01, -3.4399e-01,\n",
       "          7.5735e-01, -4.1246e-01,  6.6310e-01, -3.7722e-01,  2.5249e-01,\n",
       "          6.7867e-04,  4.9980e-01,  1.5659e-01, -4.6823e-01, -8.5926e-01,\n",
       "          2.5678e-01, -5.8305e-01,  1.1061e-01, -3.4103e-01, -5.9007e-01,\n",
       "          3.4241e-01,  3.5510e-01, -6.0619e-02,  5.1156e-01, -4.5675e-01,\n",
       "         -1.7374e-01,  2.5038e-02,  6.9296e-01,  6.3424e-01, -1.5416e-01,\n",
       "          5.3319e-01, -2.1026e-01,  6.9958e-02,  1.1054e-01, -1.2865e-01,\n",
       "         -9.1280e-02, -3.8652e-01,  6.4573e-01, -4.1565e-01, -7.1381e-02,\n",
       "          1.2075e-01,  3.7104e-01,  3.5611e-01,  3.7390e-01,  6.6714e-01,\n",
       "         -5.8333e-01,  2.0038e-02,  3.4800e-01, -1.6478e-01, -1.0651e-01,\n",
       "          2.3218e-01,  1.8154e-01, -3.0655e-01,  4.7024e-01, -2.0956e-01,\n",
       "          1.0335e-01, -1.3541e-01, -2.8075e-01, -9.0258e-02,  3.1603e-01,\n",
       "         -5.1596e-01,  7.2698e-01,  2.1331e-02,  2.6178e-01,  4.2488e-01,\n",
       "         -3.6758e-01, -3.5015e-01,  4.2474e-01, -8.3752e-02, -3.5468e-02,\n",
       "          6.5077e-01,  4.8900e-02,  2.0051e-01,  7.3786e-02,  8.2024e-01,\n",
       "         -7.1398e-01,  3.4800e-01, -1.7986e-01, -1.5111e-01,  2.2227e-02,\n",
       "         -3.7994e-01,  3.9704e-01, -4.5897e-02,  5.1540e-01, -2.3748e-01,\n",
       "          2.7915e-01, -5.7029e-01,  1.9444e-01,  3.0663e-01,  5.7417e-01,\n",
       "         -9.8612e-02,  2.1327e-01,  4.0991e-01,  5.0152e-01,  1.6415e-01,\n",
       "         -4.2796e-01, -5.5240e-02,  1.5309e-01,  3.2694e-01, -2.7457e-01,\n",
       "          4.8495e-01,  4.4859e-02, -3.8364e-01,  3.3873e-01,  4.2520e-01,\n",
       "         -6.2881e-01,  4.9344e-01, -4.3371e-02, -1.3952e-01, -1.4252e-01,\n",
       "         -2.5603e-01, -4.8396e-01, -6.8554e-01, -6.8177e-01,  2.1539e-01,\n",
       "         -2.6712e-01, -8.0814e-02, -5.5263e-01,  1.8719e-01, -2.1112e-01,\n",
       "          5.1358e-01, -1.9327e-01, -1.2401e-01,  4.3388e-01,  1.3040e-01,\n",
       "         -3.0633e-01,  6.0557e-03, -2.9106e-01,  5.2138e-01, -8.2763e-01,\n",
       "          6.0335e-02, -4.9966e-01,  4.7605e-01, -3.2770e-02,  1.9878e-01,\n",
       "         -5.0307e-01, -4.5533e-02,  1.2239e-01,  1.6808e-01,  2.2950e-01,\n",
       "         -3.4415e-01,  3.3589e-01,  3.3932e-01,  1.1404e-01,  3.0741e-02,\n",
       "          4.1581e-01, -1.1256e-01,  6.5036e-01,  6.7020e-02,  2.9813e-01,\n",
       "          8.3906e-02, -3.7784e-02,  6.0086e-01,  3.6392e-01, -1.8965e-02,\n",
       "          3.6815e-01,  3.8635e-01, -5.5352e-01, -2.9835e-01,  3.9173e-01,\n",
       "          1.7826e-01,  5.8094e-01,  4.6194e-02, -1.5625e-01,  6.4276e-01,\n",
       "          4.7954e-01, -4.2242e-01, -4.0513e-01,  6.1782e-02, -1.7882e-01,\n",
       "          1.6373e-01, -4.5150e-01, -1.8299e-01,  1.9015e-01,  6.3145e-01,\n",
       "          8.0670e-01,  9.5154e-02, -4.0402e-02, -9.1024e-02,  7.4229e-01,\n",
       "          2.4370e-01, -5.0359e-01,  2.8360e-02, -5.1451e-01, -9.3528e-02,\n",
       "         -1.9571e-01,  4.9200e-01,  3.2621e-01,  5.6070e-01, -2.8072e-01,\n",
       "          4.2890e-01,  1.0816e-01,  1.5471e-01,  1.0245e-01, -3.3704e-01,\n",
       "          4.0460e-01,  5.8528e-02,  2.9751e-01]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_config={\n",
    "        \"variational\": True, \n",
    "        \"eval_metrics\": [\"description_length\"],\n",
    "        \"lr\": 1e-3,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"n_epochs\": 100,\n",
    "        \"loss_function\": \"crossentropy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "data_path_train = ...\n",
    "data_path_val = ...\n",
    "data_path_test = ...\n",
    "\n",
    "\n",
    "train_dataset = MDLDataset_POSTagging(data_path_train)\n",
    "val_dataset = MDLDataset_POSTagging(data_path_val)\n",
    "test_dataset = MDLDataset_POSTagging(data_path_test)\n",
    "\n",
    "collator = Collator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collator)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collator)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    evaluate_every=10\n",
    ")\n",
    "\n",
    "val_metrics_mean = trainer._metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics_mean, test_metrics = trainer.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(metrics_mean): # TODO: implement and transfer to utils\n",
    "    ...\n",
    "\n",
    "\n",
    "visualize(val_metrics_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
