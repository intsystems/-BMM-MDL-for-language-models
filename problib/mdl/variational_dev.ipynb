{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Trainer import Trainer\n",
    "\n",
    "from variational_probing import VariationalProbingModel\n",
    "from utils import *\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at D:/models/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_path_train = \"conll2000_train.txt\"\n",
    "data_path_val = \"conll2000_test.txt\"\n",
    "data_path_test = \"conll2000_test.txt\"\n",
    "\n",
    "\n",
    "train_dataset = MDLDataset_POSTagging(data_path_train)\n",
    "tagging_tokenizer = train_dataset.tagging_tokenizer\n",
    "val_dataset = MDLDataset_POSTagging(data_path_val, tagging_tokenizer)\n",
    "test_dataset = MDLDataset_POSTagging(data_path_test, tagging_tokenizer)\n",
    "\n",
    "model = VariationalProbingModel(pretrained_path=\"D:/models/roberta-base\", out_features=len(tagging_tokenizer)).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/models/roberta-base\")\n",
    "\n",
    "collator = Collator(\n",
    "    tokenizer=tokenizer,\n",
    "    post_tagging_tokenizer=tagging_tokenizer,\n",
    "    max_length=1024,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 48])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = tokenizer([\"hello world!\", \"hello world!\"], return_tensors=\"pt\")\n",
    "test_output = model(inps[\"input_ids\"].to(\"cuda\"), inps[\"attention_mask\"].to(\"cuda\"))\n",
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fead90a6b8349a18be80afc21441250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bc8ef6e1a043f3888a94a485f2e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training batch:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f44b412cff84288b3c61c53bbe4b199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training batch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ae5ea8f2d04b8195e8bec5406dd29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training batch:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m      3\u001b[0m     train_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m val_metrics_mean \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39m_metrics\n",
      "File \u001b[1;32md:\\Desktop\\9сем\\BMM\\MDL_for_Language_Models\\problib\\mdl\\Trainer.py:102\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_loader, val_loader, model, n_epochs, loss_function, optimizer, variational, lr, evaluate_every)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m eval_metrics \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 102\u001b[0m     losses_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     losses\u001b[38;5;241m.\u001b[39mextend(losses_epoch)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Desktop\\9сем\\BMM\\MDL_for_Language_Models\\problib\\mdl\\Trainer.py:134\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self, train_loader, loss_function, optimizer)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    132\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(batch, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 134\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variational:\n\u001b[0;32m    137\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([m\u001b[38;5;241m.\u001b[39mkl_divergence() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m bayes_modules])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_config={\n",
    "        \"variational\": True, \n",
    "        \"eval_metrics\": [\"description_length\"],\n",
    "        \"lr\": 1e-3,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"n_epochs\": 250,\n",
    "        \"loss_function\": \"crossentropy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "model = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    evaluate_every=10\n",
    ")\n",
    "\n",
    "val_metrics_mean = trainer._metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_metrics_mean, test_metrics = trainer.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize(metrics_mean): # TODO: implement and transfer to utils\n",
    "#     ...\n",
    "\n",
    "\n",
    "# visualize(val_metrics_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
