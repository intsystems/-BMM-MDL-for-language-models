{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MDL Library: Advanced Approaches for Minimum Description Length","text":"<p>Welcome to the MDL Library, a collection of advanced approaches for working with Minimum Description Length (MDL). This library provides various methodologies to efficiently model, compress, and analyze data through the MDL principle, offering flexible options for different use cases in data science and machine learning.</p>"},{"location":"#library-structure","title":"Library Structure","text":"<p>This library includes multiple approaches, each addressing specific challenges in MDL-based modeling. The documentation is structured as follows:</p> <ul> <li>Online Coding: </li> <li>Variational Coding:</li> <li>Latent Variable Probing: </li> <li>Bayesian approach: </li> </ul> <p>Each method page provides: - Theoretical Background: An accessible overview of the theory behind the approach. - Usage Example: Code snippets to demonstrate how to apply the method effectively. - References: Links to relevant research papers for further reading.</p> <p>Dive into any approach to learn more, explore theory, and see practical examples.</p>"},{"location":"bayes/","title":"MDL: Bayesian Probing","text":""},{"location":"bayes/#overview","title":"Overview","text":"<p>Bayesian approach similar to other approaches applies information-theoretic concepts to streamline data compression and emphasize essential features. It utilises Bayesian mutual information from the perspective of Bayesian agents. For instance, under Bayesian MI various operations with data affect its representations. Similar to MDL, it can be applied to the problem of probing.</p>"},{"location":"bayes/#theoretical-background","title":"\ud83d\udcd6 Theoretical Background","text":"<p>You can dive into theoretical underpinnings in our blogpost, or dive into original paper.</p>"},{"location":"bayes/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":"<p>Below is a step-by-step guide to implementing Bayesian Agents belief approach using an MLP configuration.</p> <p>In the example, we will solve a task of POS-tagging, however it can be applied to classification task as well.</p> <pre><code>from problib.base import BaseModel\nfrom problib.bayesian.belief_agents import MLP\nfrom problib.bayesian.probing import BayesianProbingModel\n\nimport MLPConfig, ProbingModel, MLPTrainer\n\n# Define the model configuration with essential parameters\ntrain_config = MLPConfig(\n    K=5,  # Number of top features to sample\n    input_dim=768,  # Input dimensionality\n    hidden_dim=256,  # Size of hidden layers\n    output_dim=10,  # Output dimensionality (e.g., number of classes)\n    num_layers=2,  # Number of layers in the MLP\n    sampler_type=\"poisson\"  # Type of sampler, e.g., \"poisson\" or \"conditional_poisson\"\n)\n\n# Initialize the probing model with the defined configuration\nmodel = BayesianProbingModel(belief_model,\n                             train_config)\n\n# Set up the trainer for model training\ntrainer = MLPTrainer(\n    model=model,\n    train_dataset=train_dataset,  # Training dataset\n    mc_samples=5,  # Monte Carlo samples for likelihood estimation\n    entropy_scale=1e-3,  # Regularization scale for entropy\n    l1_weight=1e-5,  # Weight for L1 regularization\n    l2_weight=1e-5  # Weight for L2 regularization\n)\n\n# Train the model\ntrainer.train()\n\n# Get results of probing \nprobing_results = model.probe.get_probe_results()\nprint(\"Probing results:\", probing_results)\n</code></pre>"},{"location":"latent_var/","title":"MDL: Latent Variable Probing","text":""},{"location":"latent_var/#overview","title":"Overview","text":"<p>Approach 1 applies information-theoretic concepts to streamline data compression and emphasize essential features. Rooted in MDL (Minimum Description Length) theory, this approach is highly effective for managing large datasets and performing feature reduction. By concentrating on the most impactful features, it enhances model efficiency, making it particularly valuable for complex, high-dimensional data.</p>"},{"location":"latent_var/#theoretical-background","title":"\ud83d\udcd6 Theoretical Background","text":"<p>Put description </p>"},{"location":"latent_var/#original-research","title":"\ud83d\udcc4 Original Research","text":"<p>For an in-depth exploration of the theoretical underpinnings, you can view the paper directly below:</p>      This browser does not support PDFs. Please download the PDF to view it:      Download PDF."},{"location":"latent_var/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":"<p>Below is a step-by-step guide to implementing Latent Variable approach using an MLP configuration.</p> <pre><code>from mymodule import MLPConfig, ProbingModel, MLPTrainer\n\n# Define the model configuration with essential parameters\nconfig = MLPConfig(\n    K=5,  # Number of top features to sample\n    input_dim=768,  # Input dimensionality\n    hidden_dim=256,  # Size of hidden layers\n    output_dim=10,  # Output dimensionality (e.g., number of classes)\n    num_layers=2,  # Number of layers in the MLP\n    sampler_type=\"poisson\"  # Type of sampler, e.g., \"poisson\" or \"conditional_poisson\"\n)\n\n# Initialize the probing model with the defined configuration\nmodel = ProbingModel(config)\n\n# Set up the trainer for model training\ntrainer = MLPTrainer(\n    model=model,\n    train_dataset=train_dataset,  # Training dataset\n    mc_samples=5,  # Monte Carlo samples for likelihood estimation\n    entropy_scale=1e-3,  # Regularization scale for entropy\n    l1_weight=1e-5,  # Weight for L1 regularization\n    l2_weight=1e-5  # Weight for L2 regularization\n)\n\n# Train the model\ntrainer.train()\n\n# Retrieve the top K features based on learned sampler weights\ntop_k_features = model.sampler.get_top_k_features()\nprint(\"Top K features:\", top_k_features)\n</code></pre>"}]}