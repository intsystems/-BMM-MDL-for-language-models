{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MDL Library: Advanced Approaches for Minimum Description Length","text":"<p>Welcome to the MDL Library, a collection of advanced approaches for working with Minimum Description Length (MDL). This library provides various methodologies to efficiently model, compress, and analyze data through the MDL principle, offering flexible options for different use cases in data science and machine learning.</p>"},{"location":"#library-structure","title":"Library Structure","text":"<p>This library includes multiple approaches, each addressing specific challenges in MDL-based modeling. The documentation is structured as follows:</p> <ul> <li>Online Coding: </li> <li>Variational Coding:</li> <li>Latent Variable Probing: </li> <li>Bayesian approach: </li> </ul> <p>Each method page provides: - Theoretical Background: An accessible overview of the theory behind the approach. - Usage Example: Code snippets to demonstrate how to apply the method effectively. - References: Links to relevant research papers for further reading.</p> <p>Dive into any approach to learn more, explore theory, and see practical examples.</p>"},{"location":"bayes/","title":"MDL: Bayesian Probing","text":""},{"location":"bayes/#overview","title":"Overview","text":"<p>Bayesian approach similar to other approaches applies information-theoretic concepts to streamline data compression and emphasize essential features. It utilises Bayesian mutual information from the perspective of Bayesian agents. For instance, under Bayesian MI various operations with data affect its representations. Similar to MDL, it can be applied to the problem of probing.</p>"},{"location":"bayes/#theoretical-background","title":"\ud83d\udcd6 Theoretical Background","text":"<p>You can dive into theoretical underpinnings in our blogpost, or dive into original paper.</p>"},{"location":"bayes/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Explore the components of the model used in this approach, organized into sections for clarity.</p>"},{"location":"bayes/#model-architecture","title":"\ud83e\udde0 Model Architecture","text":"<p>The Bayesian Probing Model integrates an MLP classifier with a sampling mechanism.</p>"},{"location":"bayes/#problib.bayesian.probing","title":"<code>problib.bayesian.probing</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel","title":"<code>BayesianProbingModel(embedding_size: int = 512, n_classes: int = 2, hidden_size: int = 512, n_layers: int = 10, dropout: float = 0.1, representation: str = None, n_words: int = 10, device: str = 'cuda')</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bayesian Probing Model that utilizes a multi-layer perceptron (MLP) for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <p>Base class for models in this library.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>MLP</code> <p>The MLP model used for classification.</p> <code>name</code> <code>str</code> <p>Identifier for the model (\"mlp\").</p> <p>Initialize the Bayesian Probing Model.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The size of the embedding layer.</p> <code>512</code> <code>int</code> <p>The number of output classes.</p> <code>2</code> <code>int</code> <p>The size of the hidden layers.</p> <code>512</code> <code>int</code> <p>The number of layers in the MLP.</p> <code>10</code> <code>float</code> <p>The dropout rate for regularization.</p> <code>0.1</code> <code>str</code> <p>The type of representation to use. Defaults to None.</p> <code>None</code> <code>int</code> <p>The number of words in the vocabulary.</p> <code>10</code> <code>str</code> <p>The device to run the model on (e.g., \"cuda\" or \"cpu\").</p> <code>'cuda'</code>"},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(BaseModel)","title":"<code>BaseModel</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(embedding_size)","title":"<code>embedding_size</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(n_classes)","title":"<code>n_classes</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(hidden_size)","title":"<code>hidden_size</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(n_layers)","title":"<code>n_layers</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(dropout)","title":"<code>dropout</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(representation)","title":"<code>representation</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(n_words)","title":"<code>n_words</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel(device)","title":"<code>device</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.name","title":"<code>name = 'mlp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.model","title":"<code>model = MLP(embedding_size=embedding_size, n_classes=n_classes, hidden_size=hidden_size, nlayers=n_layers, dropout=dropout, representation=representation, n_words=n_words)</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.forward","title":"<code>forward(input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>The input IDs of the tokens.</p> required <code>Optional[Tensor]</code> <p>The attention mask for the tokens. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output logits from the model.</p>"},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.forward(input_ids)","title":"<code>input_ids</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.forward(attention_mask)","title":"<code>attention_mask</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.evaluate","title":"<code>evaluate(evalloader: torch.utils.data.DataLoader, model) -&gt; Dict[str, float]</code>","text":"<p>Evaluate the model on the evaluation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>DataLoader</code> <p>The evaluation dataset loader.</p> required <code>BayesianProbingModel</code> <p>The model to evaluate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary containing the evaluation loss and accuracy.</p>"},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.evaluate(evalloader)","title":"<code>evalloader</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.evaluate(model)","title":"<code>model</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch","title":"<code>train_epoch(trainloader: torch.utils.data.DataLoader, devloader: torch.utils.data.DataLoader, model, optimizer: optim.Optimizer, train_info: TrainInfo)</code>","text":"<p>Train the model for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>DataLoader</code> <p>The training dataset loader.</p> required <code>DataLoader</code> <p>The development dataset loader.</p> required <code>BayesianProbingModel</code> <p>The model to train.</p> required <code>Optimizer</code> <p>The optimizer to use for training.</p> required <code>TrainInfo</code> <p>Object containing training information and progress tracking.</p> required"},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch(trainloader)","title":"<code>trainloader</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch(devloader)","title":"<code>devloader</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch(model)","title":"<code>model</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch(optimizer)","title":"<code>optimizer</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train_epoch(train_info)","title":"<code>train_info</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train","title":"<code>train(trainloader: torch.utils.data.DataLoader, devloader: torch.utils.data.DataLoader, model, eval_batches: int, wait_iterations: int)</code>","text":"<p>Train the model over multiple epochs until completion or stopping criteria are met.</p> <p>Parameters:</p> Name Type Description Default <code>DataLoader</code> <p>The training dataset loader.</p> required <code>DataLoader</code> <p>The development dataset loader.</p> required <code>BayesianProbingModel</code> <p>The model to train.</p> required <code>int</code> <p>Number of batches to evaluate after each epoch.</p> required <code>int</code> <p>Number of iterations to wait before stopping early.</p> required <p>This method manages the overall training loop and progress reporting using tqdm for visualization.</p>"},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train(trainloader)","title":"<code>trainloader</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train(devloader)","title":"<code>devloader</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train(model)","title":"<code>model</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train(eval_batches)","title":"<code>eval_batches</code>","text":""},{"location":"bayes/#problib.bayesian.probing.BayesianProbingModel.train(wait_iterations)","title":"<code>wait_iterations</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model","title":"<code>problib.bayesian.bayesian_model</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig","title":"<code>MLPConfig(K: int, input_dim: int = 768, hidden_dim: int = 256, output_dim: int = 3, num_layers: int = 2, dropout: float = 0.1, vocab_size: int = 30522, **kwargs)</code>","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>Configuration class for the MLPClassifier model.</p> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>str</code> <p>Type of the model (default: \"mlp_classifier\").</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> <code>hidden_dim</code> <code>int</code> <p>Dimensionality of the hidden layers.</p> <code>output_dim</code> <code>int</code> <p>Number of output classes.</p> <code>num_layers</code> <code>int</code> <p>Number of hidden layers in the MLP.</p> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization.</p> <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for embedding.</p> <p>Initialize the MLPConfig object.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of samples or features to consider.</p> required <code>int</code> <p>Dimensionality of the input features. Defaults to 768.</p> <code>768</code> <code>int</code> <p>Dimensionality of the hidden layers. Defaults to 256.</p> <code>256</code> <code>int</code> <p>Number of output classes. Defaults to 3.</p> <code>3</code> <code>int</code> <p>Number of hidden layers in the MLP. Defaults to 2.</p> <code>2</code> <code>float</code> <p>Dropout rate for regularization. Defaults to 0.1.</p> <code>0.1</code> <code>int</code> <p>Size of the vocabulary for embedding. Defaults to 30522.</p> <code>30522</code> <p>Additional keyword arguments for PretrainedConfig initialization.</p> <code>{}</code>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(K)","title":"<code>K</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(input_dim)","title":"<code>input_dim</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(hidden_dim)","title":"<code>hidden_dim</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(output_dim)","title":"<code>output_dim</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(num_layers)","title":"<code>num_layers</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(dropout)","title":"<code>dropout</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(vocab_size)","title":"<code>vocab_size</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.model_type","title":"<code>model_type = 'mlp_classifier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.input_dim","title":"<code>input_dim = input_dim</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.hidden_dim","title":"<code>hidden_dim = hidden_dim</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.output_dim","title":"<code>output_dim = output_dim</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.num_layers","title":"<code>num_layers = num_layers</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.D","title":"<code>D = input_dim</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.K","title":"<code>K = K</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.dropout","title":"<code>dropout = dropout</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLPConfig.vocab_size","title":"<code>vocab_size = vocab_size</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP","title":"<code>MLP(task: str, config: MLPConfig)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Multi-layer Perceptron model.</p> <p>This model consists of multiple linear layers followed by ReLU activations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name identifier for the model (\"mlp\").</p> <code>mlp</code> <code>Sequential</code> <p>Sequential container for MLP layers.</p> <code>out</code> <code>Linear</code> <p>Output layer mapping from hidden size to output dimension.</p> <code>dropout</code> <code>Dropout</code> <p>Dropout layer for regularization during training.</p> <p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The task to perform (e.g., \"dep_label\").</p> required <code>MLPConfig</code> <p>The configuration for the model.</p> required"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP(task)","title":"<code>task</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP(config)","title":"<code>config</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.name","title":"<code>name = 'mlp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.mlp","title":"<code>mlp = self.build_mlp()</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.out","title":"<code>out = nn.Linear(self.final_hidden_size, config.output_dim)</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.dropout","title":"<code>dropout = nn.Dropout(config.dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.criterion","title":"<code>criterion = nn.CrossEntropyLoss()</code>  <code>instance-attribute</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.build_embeddings","title":"<code>build_embeddings(n_words: int, embedding_size: int)</code>","text":"<p>Build the embeddings for the model.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The number of words in vocabulary.</p> required <code>int</code> <p>The size of each embedding vector.</p> required <p>This method initializes an embedding layer based on task requirements and representation type.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.build_embeddings(n_words)","title":"<code>n_words</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.build_embeddings(embedding_size)","title":"<code>embedding_size</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.build_mlp","title":"<code>build_mlp()</code>","text":"<p>Builds the MLP architecture based on configuration parameters.</p> <p>Returns:</p> Type Description <p>nn.Sequential: A sequential container with linear layers and activations configured according to num_layers and dropout settings.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.forward","title":"<code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>The input tensor with shape [batch_size, ...].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output logits from the model.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.forward(x)","title":"<code>x</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.get_embeddings","title":"<code>get_embeddings(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Get embeddings for input data.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor containing indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The resulting embeddings after processing through embedding layer.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.get_embeddings(x)","title":"<code>x</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.train_batch","title":"<code>train_batch(data: torch.Tensor, target: torch.Tensor, optimizer)</code>","text":"<p>Train the model on a single batch.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor containing data samples.</p> required <code>Tensor</code> <p>Target tensor containing labels corresponding to data samples.</p> required <code>Optimizer</code> <p>Optimizer instance used for parameter updates.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The computed loss value after training on this batch.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.train_batch(data)","title":"<code>data</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.train_batch(target)","title":"<code>target</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.train_batch(optimizer)","title":"<code>optimizer</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.eval_batch","title":"<code>eval_batch(data: torch.Tensor, target: torch.Tensor) -&gt; Tuple[float, float]</code>","text":"<p>Evaluate performance on a single batch.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor containing data samples for evaluation.</p> required <code>Tensor</code> <p>Target tensor containing labels corresponding to data samples.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float]</code> <p>A tuple containing total loss and accuracy over this batch.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.eval_batch(data)","title":"<code>data</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.eval_batch(target)","title":"<code>target</code>","text":""},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.get_norm","title":"<code>get_norm() -&gt; torch.Tensor</code>  <code>staticmethod</code>","text":"<p>Get norm value associated with this model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor representing norm value; placeholder implementation here returns zero tensor.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.get_args","title":"<code>get_args() -&gt; Dict[str, Union[int, str]]</code>","text":"<p>Get arguments relevant to this model configuration.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[int, str]]</code> <p>A dictionary containing relevant configuration arguments as key-value pairs.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.print_param_names","title":"<code>print_param_names() -&gt; list</code>  <code>staticmethod</code>","text":"<p>Print parameter names relevant to this model configuration.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of parameter names as strings.</p>"},{"location":"bayes/#problib.bayesian.bayesian_model.MLP.print_params","title":"<code>print_params() -&gt; list</code>","text":"<p>Print parameters relevant to this model configuration.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing current parameter values as stored in attributes.</p>"},{"location":"bayes/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":"<p>Below is a step-by-step guide to implementing Bayesian Agents belief approach using an MLP configuration.</p> <p>In the example, we will solve a task of POS-tagging, however it can be applied to classification task as well.</p> <pre><code>from problib.base import BaseModel\nfrom problib.bayesian.belief_agents import MLP\nfrom problib.bayesian.probing import BayesianProbingModel\n\nimport MLPConfig, ProbingModel, MLPTrainer\n\n# Define the model configuration with essential parameters\ntrain_config = MLPConfig(\n    K=5,  # Number of top features to sample\n    input_dim=768,  # Input dimensionality\n    hidden_dim=256,  # Size of hidden layers\n    output_dim=10,  # Output dimensionality (e.g., number of classes)\n    num_layers=2,  # Number of layers in the MLP\n    sampler_type=\"poisson\"  # Type of sampler, e.g., \"poisson\" or \"conditional_poisson\"\n)\n\n# Initialize the probing model with the defined configuration\nmodel = BayesianProbingModel(belief_model,\n                             train_config)\n\n# Set up the trainer for model training\ntrainer = MLPTrainer(\n    model=model,\n    train_dataset=train_dataset,  # Training dataset\n    mc_samples=5,  # Monte Carlo samples for likelihood estimation\n    entropy_scale=1e-3,  # Regularization scale for entropy\n    l1_weight=1e-5,  # Weight for L1 regularization\n    l2_weight=1e-5  # Weight for L2 regularization\n)\n\n# Train the model\ntrainer.train()\n\n# Get results of probing \nprobing_results = model.probe.get_probe_results()\nprint(\"Probing results:\", probing_results)\n</code></pre>"},{"location":"latent_var/","title":"MDL: Latent Variable Probing","text":""},{"location":"latent_var/#overview","title":"Overview","text":"<p>Approach 1 applies information-theoretic concepts to streamline data compression and emphasize essential features. Rooted in MDL (Minimum Description Length) theory, this approach is highly effective for managing large datasets and performing feature reduction. By concentrating on the most impactful features, it enhances model efficiency, making it particularly valuable for complex, high-dimensional data.</p>"},{"location":"latent_var/#theoretical-background","title":"\ud83d\udcd6 Theoretical Background","text":"<p>For an in-depth exploration of the theoretical underpinnings, you can view the paper:</p>"},{"location":"latent_var/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Explore the components of the model used in this approach, organized into sections for clarity.</p>"},{"location":"latent_var/#model-architecture","title":"\ud83e\udde0 Model Architecture","text":"<p>Detailed documentation of the MLP (Multi-Layer Perceptron) model used for latent variable modeling.</p>"},{"location":"latent_var/#problib.latent_var.modeling_mlp","title":"<code>problib.latent_var.modeling_mlp</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig","title":"<code>MLPConfig(K: int, input_dim: int = 768, hidden_dim: int = 256, output_dim: int = 10, num_layers: int = 2, sampler_type: str = 'poisson', **kwargs)</code>","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>Configuration class for the MLPClassifier model.</p> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>str</code> <p>Type of the model (default: \"mlp_classifier\").</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> <code>hidden_dim</code> <code>int</code> <p>Dimensionality of the hidden layers.</p> <code>output_dim</code> <code>int</code> <p>Number of output classes.</p> <code>num_layers</code> <code>int</code> <p>Number of hidden layers in the MLP.</p> <code>sampler_type</code> <code>str</code> <p>Type of sampler to use.</p> <code>D</code> <code>int</code> <p>Dimensionality of the input features (alias for input_dim).</p> <code>K</code> <code>int</code> <p>Number of samples or features to consider.</p> <p>Initialize the MLPConfig object.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of samples or features to consider.</p> required <code>int</code> <p>Dimensionality of the input features. Defaults to 768.</p> <code>768</code> <code>int</code> <p>Dimensionality of the hidden layers. Defaults to 256.</p> <code>256</code> <code>int</code> <p>Number of output classes. Defaults to 10.</p> <code>10</code> <code>int</code> <p>Number of hidden layers in the MLP. Defaults to 2.</p> <code>2</code> <code>str</code> <p>Type of sampler to use. Defaults to \"poisson\".</p> <code>'poisson'</code> <p>Additional keyword arguments for PretrainedConfig initialization.</p> <code>{}</code>"},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(K)","title":"<code>K</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(input_dim)","title":"<code>input_dim</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(hidden_dim)","title":"<code>hidden_dim</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(output_dim)","title":"<code>output_dim</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(num_layers)","title":"<code>num_layers</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(sampler_type)","title":"<code>sampler_type</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.model_type","title":"<code>model_type = 'mlp_classifier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.input_dim","title":"<code>input_dim = input_dim</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.hidden_dim","title":"<code>hidden_dim = hidden_dim</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.output_dim","title":"<code>output_dim = output_dim</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.num_layers","title":"<code>num_layers = num_layers</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.sampler_type","title":"<code>sampler_type = sampler_type</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.D","title":"<code>D = input_dim</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPConfig.K","title":"<code>K = K</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier","title":"<code>MLPClassifier(config: MLPConfig)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Layer Perceptron Classifier model.</p> <p>This model consists of multiple linear layers followed by ReLU activations.</p> <p>Parameters:</p> Name Type Description Default <p>Base class for all neural network modules in PyTorch.</p> required <p>Attributes:</p> Name Type Description <code>layers</code> <code>Sequential</code> <p>Sequential container for the MLP layers.</p> <p>Initialize the MLPClassifier model.</p> <p>Parameters:</p> Name Type Description Default <code>MLPConfig</code> <p>Configuration object containing model parameters.</p> required"},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier(nn.Module)","title":"<code>nn.Module</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier(config)","title":"<code>config</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier.layers","title":"<code>layers = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier.forward","title":"<code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Forward pass of the MLPClassifier model.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor with shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output logits with shape (batch_size, output_dim).</p>"},{"location":"latent_var/#problib.latent_var.modeling_mlp.MLPClassifier.forward(x)","title":"<code>x</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel","title":"<code>ProbingModel(config: MLPConfig)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Probing Model for analysis.</p> <p>This model integrates an MLP classifier with a sampling mechanism.</p> <p>Parameters:</p> Name Type Description Default <p>Base class for models in this library.</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name identifier for the probing model.</p> <code>config</code> <code>MLPConfig</code> <p>Configuration object containing model parameters.</p> <code>sampler</code> <p>Sampler instance used for generating masks.</p> <code>classifier</code> <code>MLPClassifier</code> <p>Instance of the MLPClassifier used for predictions.</p> <p>Initialize the ProbingModel.</p> <p>Parameters:</p> Name Type Description Default <code>MLPConfig</code> <p>Configuration object containing model parameters.</p> required"},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel(BaseModel)","title":"<code>BaseModel</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel(config)","title":"<code>config</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.name","title":"<code>name = 'probing_model'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.sampler","title":"<code>sampler = get_sampler(config.sampler_type, config.D, config.K)</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.classifier","title":"<code>classifier = MLPClassifier(config)</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.forward","title":"<code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Forward pass of the ProbingModel.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor with shape (batch_size, D).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output logits after applying sampling and classification.</p> <p>The method samples a mask from the sampler and applies it to the input tensor before passing it through  the classifier. The resulting logits are returned as output.</p>"},{"location":"latent_var/#problib.latent_var.modeling_mlp.ProbingModel.forward(x)","title":"<code>x</code>","text":""},{"location":"latent_var/#training-and-probing","title":"\ud83c\udfaf Training and Probing","text":"<p>Comprehensive guide on the trainer used to probe and train the latent variable model.</p>"},{"location":"latent_var/#problib.latent_var.probe_trainer","title":"<code>problib.latent_var.probe_trainer</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer","title":"<code>MLPTrainer(model: nn.Module, mc_samples: int = 5, entropy_scale: float = 0.001, l1_weight: float = 0.0, l2_weight: float = 0.0, *args, **kwargs)</code>","text":"<p>               Bases: <code>Trainer</code></p> <p>Custom Trainer class for training MLP models with additional loss computations.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The neural network model to train.</p> required <p>Base Trainer class from the transformers library.</p> required <p>Initializes the MLPTrainer with specified model and training parameters.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The neural network model to train.</p> required <code>int</code> <p>Number of Monte Carlo samples for loss computation.</p> <code>5</code> <code>float</code> <p>Scale factor for entropy loss.</p> <code>0.001</code> <code>float</code> <p>Weight for L1 regularization.</p> <code>0.0</code> <code>float</code> <p>Weight for L2 regularization.</p> <code>0.0</code> <p>Additional positional arguments for Trainer.</p> <code>()</code> <p>Additional keyword arguments for Trainer.</p> <code>{}</code>"},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(model)","title":"<code>model</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(Trainer)","title":"<code>Trainer</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(model)","title":"<code>model</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(mc_samples)","title":"<code>mc_samples</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(entropy_scale)","title":"<code>entropy_scale</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(l1_weight)","title":"<code>l1_weight</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(l2_weight)","title":"<code>l2_weight</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(*args)","title":"<code>*args</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.mc_samples","title":"<code>mc_samples = mc_samples</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.entropy_scale","title":"<code>entropy_scale = entropy_scale</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.l1_weight","title":"<code>l1_weight = l1_weight</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.l2_weight","title":"<code>l2_weight = l2_weight</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.sampler","title":"<code>sampler = model.sampler</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.compute_loss","title":"<code>compute_loss(model: nn.Module, inputs: Dict[str, torch.Tensor], return_outputs: Optional[bool] = False) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</code>","text":"<p>Compute the total loss for training.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The neural network model.</p> required <code>Dict[str, Tensor]</code> <p>Input dictionary containing 'input_ids' and 'labels'.</p> required <code>Optional[bool]</code> <p>Whether to return outputs along with loss.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, Tensor]]</code> <p>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: Computed loss or loss and logits.</p>"},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.compute_loss(model)","title":"<code>model</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.compute_loss(inputs)","title":"<code>inputs</code>","text":""},{"location":"latent_var/#problib.latent_var.probe_trainer.MLPTrainer.compute_loss(return_outputs)","title":"<code>return_outputs</code>","text":""},{"location":"latent_var/#samplers","title":"\ud83c\udfb2 Samplers","text":"<p>Overview of the sampling methods integrated into this approach.</p>"},{"location":"latent_var/#problib.latent_var.samplers","title":"<code>problib.latent_var.samplers</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.BaseSampler","title":"<code>BaseSampler(D: int, K: int)</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"latent_var/#problib.latent_var.samplers.BaseSampler.D","title":"<code>D = D</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.BaseSampler.K","title":"<code>K = K</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.BaseSampler.sample","title":"<code>sample(N: int) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.BaseSampler.get_top_k_features","title":"<code>get_top_k_features() -&gt; List[int]</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler","title":"<code>ConditionalPoissonSampler(D: int, K: int)</code>","text":"<p>               Bases: <code>BaseSampler</code></p>"},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.weights","title":"<code>weights = nn.Parameter(torch.rand(D))</code>  <code>instance-attribute</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.get_device","title":"<code>get_device() -&gt; torch.device</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.run_on_semiring","title":"<code>run_on_semiring(sr, cache: Optional[List[torch.Tensor]]) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.entropy","title":"<code>entropy(cache: Optional[List[torch.Tensor]] = None) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.logZ","title":"<code>logZ(cache: Optional[List[torch.Tensor]] = None) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.logprob","title":"<code>logprob(mask: torch.Tensor, logZ: Optional[torch.Tensor] = None) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.sample","title":"<code>sample(num_samples: int, log_cache: List[torch.Tensor]) -&gt; torch.Tensor</code>","text":""},{"location":"latent_var/#problib.latent_var.samplers.ConditionalPoissonSampler.get_top_k_features","title":"<code>get_top_k_features() -&gt; List[int]</code>","text":""},{"location":"latent_var/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":"<p>Below is a step-by-step guide to implementing Latent Variable approach using an MLP configuration.</p> <pre><code>from problib.latent_var import MLPConfig, ProbingModel, MLPTrainer\n\n# Define the model configuration with essential parameters\nconfig = MLPConfig(\n    K=5,  # Number of top features to sample\n    input_dim=768,  # Input dimensionality\n    hidden_dim=256,  # Size of hidden layers\n    output_dim=10,  # Output dimensionality (e.g., number of classes)\n    num_layers=2,  # Number of layers in the MLP\n    sampler_type=\"poisson\"  # Type of sampler, e.g., \"poisson\" or \"conditional_poisson\"\n)\n\n# Initialize the probing model with the defined configuration\nmodel = ProbingModel(config)\n\n# Set up the trainer for model training\ntrainer = MLPTrainer(\n    model=model,\n    train_dataset=train_dataset,  # Training dataset\n    mc_samples=5,  # Monte Carlo samples for likelihood estimation\n    entropy_scale=1e-3,  # Regularization scale for entropy\n    l1_weight=1e-5,  # Weight for L1 regularization\n    l2_weight=1e-5  # Weight for L2 regularization\n)\n\n# Train the model\ntrainer.train()\n\n# Retrieve the top K features based on learned sampler weights\ntop_k_features = model.sampler.get_top_k_features()\nprint(\"Top K features:\", top_k_features)\n</code></pre>"},{"location":"variational/","title":"MDL: Variational MDL probing","text":""},{"location":"variational/#overview","title":"Overview","text":"<p>Probing implementation via MDL approach, using variational dropout instead of last linear layer.  That is theoretically equivalent to learning parameters of the model, minimizing the total description lengths of the data encoded with that model and model parameters.</p>"},{"location":"variational/#theoretical-background","title":"\ud83d\udcd6 Theoretical Background","text":"<p>See 2.2.1 Variatonal Code in original paper.</p>"},{"location":"variational/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Explore the components of the model used in this approach, organized into sections for clarity.</p>"},{"location":"variational/#model-architecture","title":"\ud83e\udde0 Model Architecture","text":"<p>Detailed documentation of the MLP (Multi-Layer Perceptron) model used for variational modeling.</p>"},{"location":"variational/#problib.mdl.variational_probing","title":"<code>problib.mdl.variational_probing</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel","title":"<code>VariationalProbingModel(pretrained_path='D:/models/roberta-base', out_features=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>A variational probing model built on a pre-trained transformer backbone with a Bayesian probing layer.</p> <p>This model integrates a <code>LinearGroupNJ</code> layer as the probing layer to add a Bayesian component to the architecture.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the pre-trained transformer model. Defaults to \"D:/models/roberta-base\".</p> <code>'D:/models/roberta-base'</code> <code>int</code> <p>Number of output features for the probing layer. Defaults to the output dimension of the pooler dense layer in the backbone model.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModel</code> <p>The pre-trained transformer model as the backbone.</p> <code>probing_layer</code> <code>LinearGroupNJ</code> <p>The Bayesian probing layer.</p>"},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel(pretrained_path)","title":"<code>pretrained_path</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel(out_features)","title":"<code>out_features</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.model","title":"<code>model = AutoModel.from_pretrained(pretrained_path)</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.probing_layer","title":"<code>probing_layer = LinearGroupNJ(self.model.pooler.dense.in_features, out_features)</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.forward","title":"<code>forward(input_ids, attention_mask=None)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input token IDs for the transformer model.</p> required <code>Tensor</code> <p>Attention mask for the input tokens. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: Output of the probing layer applied to the transformer's last hidden state.</p>"},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.forward(input_ids)","title":"<code>input_ids</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.forward(attention_mask)","title":"<code>attention_mask</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.kl_divergence","title":"<code>kl_divergence()</code>","text":"<p>Computes the KL divergence for the probing layer.</p> <p>Returns:</p> Type Description <p>torch.Tensor: KL divergence value for the Bayesian probing layer.</p>"},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.train","title":"<code>train(*args, **kwargs)</code>","text":"<p>Puts the model into training mode.</p> Notes <p>Sets the probing layer to non-deterministic mode for variational training.</p> <p>Parameters:</p> Name Type Description Default <p>Positional arguments for the training method.</p> <code>()</code> <p>Keyword arguments for the training method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>nn.Module: The model in training mode.</p>"},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.train(*args)","title":"<code>*args</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.train(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.eval","title":"<code>eval(*args, **kwargs)</code>","text":"<p>Puts the model into evaluation mode.</p> Notes <p>Sets the probing layer to deterministic mode for evaluation.</p> <p>Parameters:</p> Name Type Description Default <p>Positional arguments for the evaluation method.</p> <code>()</code> <p>Keyword arguments for the evaluation method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>nn.Module: The model in evaluation mode.</p>"},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.eval(*args)","title":"<code>*args</code>","text":""},{"location":"variational/#problib.mdl.variational_probing.VariationalProbingModel.eval(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"variational/#training-and-probing","title":"\ud83c\udfaf Training and Probing","text":"<p>Comprehensive guide on the trainer used to probe and train the variational model.</p>"},{"location":"variational/#problib.mdl.Trainer","title":"<code>problib.mdl.Trainer</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer","title":"<code>Trainer(model=None, train_config=None)</code>","text":"<p>Trainer class for training models with various configurations and handling evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The model to be trained. Defaults to None.</p> <code>None</code> <code>dict</code> <p>Configuration dictionary for training. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>The model being trained.</p> <code>train_config</code> <code>dict</code> <p>Configuration settings for the training process.</p> <p>Initialize the Trainer object.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The model to train. Defaults to None.</p> <code>None</code> <code>dict</code> <p>Training configuration dictionary. Defaults to None.</p> <code>None</code>"},{"location":"variational/#problib.mdl.Trainer.Trainer(model)","title":"<code>model</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer(train_config)","title":"<code>train_config</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer(model)","title":"<code>model</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer(train_config)","title":"<code>train_config</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train_config","title":"<code>train_config = train_config if train_config is not None else {}</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train","title":"<code>train(train_loader=None, val_loader=None, model=None, n_epochs=None, loss_function=None, optimizer=None, variational=None, lr=None, evaluate_every=-1)</code>","text":"<p>Train the model using the provided configuration and data loaders.</p> <p>Parameters:</p> Name Type Description Default <code>DataLoader</code> <p>DataLoader for the training data. Defaults to None.</p> <code>None</code> <code>DataLoader</code> <p>DataLoader for validation data. Defaults to None.</p> <code>None</code> <code>Module</code> <p>Model to train. Defaults to None.</p> <code>None</code> <code>int</code> <p>Number of training epochs. Defaults to None.</p> <code>None</code> <code>str or callable</code> <p>Loss function for training. Defaults to None.</p> <code>None</code> <code>Optimizer or str</code> <p>Optimizer to use. Defaults to None.</p> <code>None</code> <code>bool</code> <p>If True, uses variational training. Defaults to None.</p> <code>None</code> <code>float</code> <p>Learning rate for the optimizer. Defaults to None.</p> <code>None</code> <code>int</code> <p>Evaluation frequency (in epochs). Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Type Description <p>nn.Module: The trained model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no model or training loader is provided.</p> <code>ValueError</code> <p>If an unknown optimizer or loss function is specified.</p>"},{"location":"variational/#problib.mdl.Trainer.Trainer.train(train_loader)","title":"<code>train_loader</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(val_loader)","title":"<code>val_loader</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(model)","title":"<code>model</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(n_epochs)","title":"<code>n_epochs</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(loss_function)","title":"<code>loss_function</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(optimizer)","title":"<code>optimizer</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(variational)","title":"<code>variational</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(lr)","title":"<code>lr</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.train(evaluate_every)","title":"<code>evaluate_every</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.evaluate","title":"<code>evaluate(val_loader=None, loss_function=None)</code>","text":"<p>Evaluate the model on the validation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>DataLoader</code> <p>Validation DataLoader. Defaults to None.</p> <code>None</code> <code>callable or str</code> <p>Loss function for evaluation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Average metrics over the validation dataset.</p>"},{"location":"variational/#problib.mdl.Trainer.Trainer.evaluate(val_loader)","title":"<code>val_loader</code>","text":""},{"location":"variational/#problib.mdl.Trainer.Trainer.evaluate(loss_function)","title":"<code>loss_function</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers","title":"<code>problib.mdl.BayesianLayers</code>","text":"<p>Variational Dropout implementation for linear layers and Bayesian neural networks.</p> <p>This module provides a <code>LinearGroupNJ</code> layer that implements Group Variational Dropout, as well as utilities for working with Bayesian layers.</p> References <p>[1] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" NIPS (2015). [2] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational Dropout Sparsifies Deep Neural Networks.\" ICML (2017). [3] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian Compression for Deep Learning.\" NIPS (2017).</p>"},{"location":"variational/#problib.mdl.BayesianLayers.BAYESIAN_LAYERS","title":"<code>BAYESIAN_LAYERS = LinearGroupNJ</code>  <code>module-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ","title":"<code>LinearGroupNJ(in_features, out_features, cuda=False, init_weight=None, init_bias=None, clip_var=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully Connected Group Normal-Jeffrey's (Group Variational Dropout) layer.</p> <p>Implements variational dropout for sparsifying neural networks.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of input features.</p> required <code>int</code> <p>Number of output features.</p> required <code>bool</code> <p>Whether to use CUDA. Defaults to False.</p> <code>False</code> <code>Tensor</code> <p>Initial weight tensor. Defaults to None.</p> <code>None</code> <code>Tensor</code> <p>Initial bias tensor. Defaults to None.</p> <code>None</code> <code>float</code> <p>Maximum variance for clipping. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>z_mu</code> <code>Parameter</code> <p>Mean for the variational dropout rates.</p> <code>z_logvar</code> <code>Parameter</code> <p>Log variance for the variational dropout rates.</p> <code>weight_mu</code> <code>Parameter</code> <p>Mean for the weight distribution.</p> <code>weight_logvar</code> <code>Parameter</code> <p>Log variance for the weight distribution.</p> <code>bias_mu</code> <code>Parameter</code> <p>Mean for the bias distribution.</p> <code>bias_logvar</code> <code>Parameter</code> <p>Log variance for the bias distribution.</p>"},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(in_features)","title":"<code>in_features</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(out_features)","title":"<code>out_features</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(cuda)","title":"<code>cuda</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(init_weight)","title":"<code>init_weight</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(init_bias)","title":"<code>init_bias</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ(clip_var)","title":"<code>clip_var</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.cuda","title":"<code>cuda = cuda</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.in_features","title":"<code>in_features = in_features</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.out_features","title":"<code>out_features = out_features</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.clip_var","title":"<code>clip_var = clip_var</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.deterministic","title":"<code>deterministic = False</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.z_mu","title":"<code>z_mu = Parameter(torch.Tensor(in_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.z_logvar","title":"<code>z_logvar = Parameter(torch.Tensor(in_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.weight_mu","title":"<code>weight_mu = Parameter(torch.Tensor(out_features, in_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.weight_logvar","title":"<code>weight_logvar = Parameter(torch.Tensor(out_features, in_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.bias_mu","title":"<code>bias_mu = Parameter(torch.Tensor(out_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.bias_logvar","title":"<code>bias_logvar = Parameter(torch.Tensor(out_features))</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.sigmoid","title":"<code>sigmoid = nn.Sigmoid()</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.softplus","title":"<code>softplus = nn.Softplus()</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.epsilon","title":"<code>epsilon = 1e-08</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.reset_parameters","title":"<code>reset_parameters(init_weight, init_bias)</code>","text":"<p>Initialize the layer parameters.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Initial weight tensor. Defaults to None.</p> required <code>Tensor</code> <p>Initial bias tensor. Defaults to None.</p> required"},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.reset_parameters(init_weight)","title":"<code>init_weight</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.reset_parameters(init_bias)","title":"<code>init_bias</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.clip_variances","title":"<code>clip_variances()</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.get_log_dropout_rates","title":"<code>get_log_dropout_rates()</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.compute_posterior_params","title":"<code>compute_posterior_params()</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the layer.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor after applying variational dropout and the linear transformation.</p>"},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.forward(x)","title":"<code>x</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.kl_divergence","title":"<code>kl_divergence()</code>","text":"<p>Compute the KL divergence for the layer.</p> <p>Returns:</p> Type Description <p>torch.Tensor: KL divergence value.</p>"},{"location":"variational/#problib.mdl.BayesianLayers.LinearGroupNJ.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.reparametrize","title":"<code>reparametrize(mu, logvar, sampling=True)</code>","text":"<p>Apply the reparametrization trick for sampling from a Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Mean of the Gaussian distribution.</p> required <code>Tensor</code> <p>Log variance of the Gaussian distribution.</p> required <code>bool</code> <p>If True, samples using reparametrization. If False, returns the mean. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: Sampled tensor or the mean if sampling is False.</p>"},{"location":"variational/#problib.mdl.BayesianLayers.reparametrize(mu)","title":"<code>mu</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.reparametrize(logvar)","title":"<code>logvar</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.reparametrize(sampling)","title":"<code>sampling</code>","text":""},{"location":"variational/#problib.mdl.BayesianLayers.get_kl_modules","title":"<code>get_kl_modules(model: nn.Module)</code>","text":"<p>Generator for iterating over Bayesian layers in a model.</p> <p>Parameters:</p> Name Type Description Default <code>Module</code> <p>The model containing Bayesian layers.</p> required <p>Yields:</p> Type Description <p>nn.Module: A Bayesian layer in the model.</p>"},{"location":"variational/#problib.mdl.BayesianLayers.get_kl_modules(model)","title":"<code>model</code>","text":""},{"location":"variational/#utils","title":"\ud83d\udee0\ufe0f Utils","text":""},{"location":"variational/#problib.mdl.utils","title":"<code>problib.mdl.utils</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer","title":"<code>POSTaggingTokenizer()</code>","text":"<p>Tokenizer for part-of-speech tagging.</p> <p>This tokenizer converts words and their part-of-speech tags into IDs and  provides functionality for encoding and decoding tags.</p> <p>Attributes:</p> Name Type Description <code>idx2tag</code> <code>dict</code> <p>Maps tag IDs to tags.</p> <code>tag2idx</code> <code>dict</code> <p>Maps tags to tag IDs.</p> <code>pad_token_id</code> <code>int</code> <p>ID for the padding token.</p> <code>bos_token_id</code> <code>int</code> <p>ID for the beginning-of-sequence token.</p> <code>eos_token_id</code> <code>int</code> <p>ID for the end-of-sequence token.</p> <code>unk_token_id</code> <code>int</code> <p>ID for the unknown token.</p> <p>Initializes the POSTaggingTokenizer with special tokens for padding,  beginning-of-sequence, end-of-sequence, and unknown tokens.</p>"},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.idx2tag","title":"<code>idx2tag = {0: '&lt;PAD&gt;', 1: '&lt;BOS&gt;', 2: '&lt;EOS&gt;', 3: '&lt;UNK&gt;'}</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.pad_token_id","title":"<code>pad_token_id = 0</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.bos_token_id","title":"<code>bos_token_id = 1</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.eos_token_id","title":"<code>eos_token_id = 2</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.unk_token_id","title":"<code>unk_token_id = 3</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.tag2idx","title":"<code>tag2idx = {'&lt;PAD&gt;': 0, '&lt;BOS&gt;': 1, '&lt;EOS&gt;': 2, '&lt;UNK&gt;': 3}</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.__len__","title":"<code>__len__()</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.fit","title":"<code>fit(sentences_conll)</code>","text":"<p>Builds the tag vocabulary from a list of sentences in the CoNLL format.</p> <p>Parameters:</p> Name Type Description Default <code>List[List[tuple]]</code> <p>List of sentences, where each  sentence is a list of (word, tag) tuples.</p> required <p>Returns:</p> Name Type Description <code>POSTaggingTokenizer</code> <p>The fitted tokenizer.</p>"},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.fit(sentences_conll)","title":"<code>sentences_conll</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.encode","title":"<code>encode(sent_words_with_tags)</code>","text":"<p>Encodes a list of words and their tags into IDs.</p> <p>Parameters:</p> Name Type Description Default <code>List[List[tuple]] or List[tuple]</code> <p>List of sentences or a single sentence, where each sentence is represented as a list of (word, tag) tuples.</p> required <p>Returns:</p> Type Description <p>List[List[tuple]] or List[tuple]: Encoded sentences or a single sentence </p> <p>with tags replaced by their IDs.</p>"},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.encode(sent_words_with_tags)","title":"<code>sent_words_with_tags</code>","text":""},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.__call__","title":"<code>__call__(NERs)</code>","text":"<p>see .encode(...)</p>"},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.decode","title":"<code>decode(ids)</code>","text":"<p>Decodes a list of tag IDs into part-of-speech tags.</p> <p>Parameters:</p> Name Type Description Default <code>list of int</code> <p>List of tag IDs.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of decoded tags.</p>"},{"location":"variational/#problib.mdl.utils.POSTaggingTokenizer.decode(ids)","title":"<code>ids</code>","text":""},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging","title":"<code>MDLDataset_POSTagging(data_path, tagging_tokenizer=None)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for part-of-speech tagging with MDL.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the dataset file. Must be \"conll2000_train.txt\" or \"conll2000_test.txt\".</p> required <code>POSTaggingTokenizer</code> <p>Tokenizer for POS tagging. If None, a new tokenizer will be created and fitted to the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>tagging_tokenizer</code> <code>POSTaggingTokenizer</code> <p>Tokenizer for encoding tags.</p> <code>data</code> <code>list</code> <p>List of encoded sentences with words and tag IDs.</p> <p>Initializes the MDLDataset_POSTagging class.</p>"},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging(data_path)","title":"<code>data_path</code>","text":""},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging(tagging_tokenizer)","title":"<code>tagging_tokenizer</code>","text":""},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging.tagging_tokenizer","title":"<code>tagging_tokenizer = tagging_tokenizer</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging.data","title":"<code>data = self._read_data(data_path)</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of samples.</p>"},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves a sample by index.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Index of the sample.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Words and corresponding tag IDs.</p>"},{"location":"variational/#problib.mdl.utils.MDLDataset_POSTagging.__getitem__(idx)","title":"<code>idx</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator","title":"<code>Collator(tokenizer, post_tagging_tokenizer, max_length=512, padding=True, truncation=True, add_special_tokens=True, **tokenizer_kwargs)</code>","text":"<p>Collator class for preparing batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>AutoTokenizer</code> <p>Tokenizer for encoding sentences into token IDs.</p> required <code>POSTaggingTokenizer</code> <p>Tokenizer for encoding POS tags into tag IDs.</p> required <code>int</code> <p>Maximum length of tokenized sequences. Defaults to 512.</p> <code>512</code> <code>bool</code> <p>Whether to pad sequences. Defaults to True.</p> <code>True</code> <code>bool</code> <p>Whether to truncate sequences. Defaults to True.</p> <code>True</code> <code>bool</code> <p>Whether to add special tokens. Defaults to True.</p> <code>True</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>AutoTokenizer</code> <p>Tokenizer for encoding sentences.</p> <code>post_tagging_tokenizer</code> <code>POSTaggingTokenizer</code> <p>Tokenizer for encoding POS tags.</p> <code>tokenizer_kwargs</code> <code>dict</code> <p>Arguments for the tokenizer.</p>"},{"location":"variational/#problib.mdl.utils.Collator(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(post_tagging_tokenizer)","title":"<code>post_tagging_tokenizer</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(max_length)","title":"<code>max_length</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(padding)","title":"<code>padding</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(truncation)","title":"<code>truncation</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(add_special_tokens)","title":"<code>add_special_tokens</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator(**tokenizer_kwargs)","title":"<code>**tokenizer_kwargs</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator.tokenizer","title":"<code>tokenizer = tokenizer</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator.post_tagging_tokenizer","title":"<code>post_tagging_tokenizer = post_tagging_tokenizer</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator.tokenizer_kwargs","title":"<code>tokenizer_kwargs = tokenizer_kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"variational/#problib.mdl.utils.Collator.__call__","title":"<code>__call__(batch)</code>","text":"<p>Prepares and tokenizes a batch of data.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <p>List of samples, where each sample is a tuple of words  and their corresponding POS tags.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Input IDs, attention masks, and POS tags per token.</p>"},{"location":"variational/#problib.mdl.utils.Collator.__call__(batch)","title":"<code>batch</code>","text":""},{"location":"variational/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":"<p>Below is a step-by-step usage example of the Variational MDL probing for POS-tagging task</p> <pre><code>from problib.mdl.variational_probing import VariationalProbingModel\nfrom problib.mdl.Trainer import Trainer\nfrom torch.utils.data import DataLoader\nfrom utils import *\nfrom transformers import AutoTokenizer\n\n# Define the model configuration with essential parameters\ntrain_config = {\n    \"variational\": True, \n    \"eval_metrics\": [\"description_length\"],\n    \"lr\": 1e-3,\n    \"optimizer\": \"Adam\",\n    \"n_epochs\": 100,\n    \"loss_function\": \"crossentropy\"\n}\n\n# Initialize the probing model with the defined configuration\nmodel = VariationalProbingModel(\n    pretrained_path=\"pretrained/model/path\",\n)\ntokenizer = AutoTokenizer.from_pretrained(\"pretrained/model/path\")\n\n# Set up the trainer for model training\ntrainer = Trainer(\n    model=model,\n    train_config=train_config\n)\n\n# get the datasets\ndata_path_train = ...\ndata_path_val = ...\ndata_path_test = ...\n\ntrain_dataset = MDLDataset_POSTagging(data_path_train)\nval_dataset = MDLDataset_POSTagging(data_path_val)\ntest_dataset = MDLDataset_POSTagging(data_path_test)\n\n# get the loaders\ncollator = Collator(\n    tokenizer=tokenizer,\n    max_length=1024,\n    padding=True,\n    truncation=True,\n    add_special_tokens=True\n)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collator)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collator)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collator)\n\n# Train the model\nmodel = trainer.train(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    evaluate_every=10\n)\n\n# Get the evaluation results\nval_metrics = trainer._metrics\n\n# Get testing results\ntest_metrics = trainer.evaluate(test_loader)\n</code></pre>"}]}